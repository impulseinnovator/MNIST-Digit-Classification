# -*- coding: utf-8 -*-
"""MNIST Digit Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ES7cqJa5DNUnbUXSovhpJmexViE97OIv
"""

import numpy as np
from pprint import pprint
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import SGDClassifier, RidgeClassifier, LogisticRegression
from sklearn.dummy import DummyRegressor
from sklearn.model_selection import cross_validate, RandomizedSearchCV, cross_val_predict
from sklearn.metrics import log_loss
from sklearn.metrics import precision_score, recall_score, classification_report
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import roc_curve,roc_auc_score

from scipy.stats import loguniform

import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)
mpl.rc('figure', figsize=(8,6))

def warn(*args, **kwargs):
  pass
import warnings
warnings.warn=warn

from sklearn.datasets import fetch_openml
x_pd, y_pd = fetch_openml('mnist_784', version=1, return_X_y=True)

X = x_pd.to_numpy()
y = y_pd.to_numpy()

scaler = MinMaxScaler()
X = scaler.fit_transform(X)

print('Mean of the features:', np.mean(X))
print('Standard deuvation  o features:', np.std(X))
print('Minimum Value:', np.min(X))
print('Maximum value:', np.max(X))

target_names = np.unique(y)
print('Numnber of samples: {0}, type:{1}'.format(X.shape[0], X.dtype))
print('Number of features: {0}.'.format(X.shape[1]))
print('Number of classes: {0}, type:{1}'.format(len(target_names), y.dtype))
print('Labels:{0}'.format(target_names))

num_images = 9
factor = np.int(np.sqrt(num_images))
fig,ax = plt.subplots(nrows=factor, ncols=factor, figsize=(8,6))
idx_offset=0
for i in range(factor):
  index=idx_offset+i*(factor)
  for j in range(factor):
    ax[i,j].imshow(X[index+j].reshape(28,28), cmap='gray')
    ax[i,j].set_title('Label:{0}'.format(str(y[index+j])))
    ax[i,j].set_axis_off()

plt.figure(figsize=(6,6))
plt.imshow(X[0].reshape(28,28), cmap='gray')
plt.show()

x_train,x_test,y_train,y_test = X[:60000],X[60000:],y[:60000],y[60000:]

plt.figure(figsize=(10,4))
sns.histplot(data=np.int8(y_train),binwidth=0.45,bins=11)
plt.xticks(ticks=[0,1,2,3,4,5,6,7,8,9], labels=[0,1,2,3,4,5,6,7,8,9])
plt.xlabel('Class')
plt.title('DSistribution of samples')
plt.show()

y_train_0 = np.zeros(len(y_train))
y_test_0 = np.zeros(len(y_test))
indx_0 = np.where(y_train == '0')
y_train_0[indx_0]=1
indx_0 = np.where(y_test == '0')
y_test_0[indx_0]=1

print(y_train)
print(y_train_0)

print(np.where(y_train== '0')
print(np.where(y_train_0 == 1))

num_images = 9
factor = np.int(np.sqrt(factor))
fig,ax = plt.subplots(nrows=factor, ncols=factor, figsize=(8,6))
idx_offset = 0
for i in range(factor):
  index = idx_offset+i*(factor)
  for j in range(factor):
    ax[i,j].imshow(X[index+j].reshape(28,28), cmap='gray')
    ax[i,j].set_title('Label:{0}'.format(str(y_train_0[index+j])))
    ax[i,j].set_axis_off()

num_pos = len(np.where(y_train_0==1)[0])
num_neg = len(np.where(y_train_0==0)[0])
print(num_pos, num_neg)

from sklearn.dummy import DummyClassifier
 base_clf = DummyClassifier(strategy='most_frequent')
 base_clf.fit(x_train, y_train_0)
 print(base_clf.score(x_train,y_train_0))

bin_sgd_clf = SGDClassifier(loss='log', penalty='l2', max_iter=1, warm_start=True, eta0=0.01, alpha=0, learning_rate='constant', random_state=1729)
Loss=[]
iterations=100
for i in range(iterations):
  bin_sgd_clf.fit(x_train, y_train_0)
  y_pred = bin_sgd_clf.predict_proba(x_train)
  Loss.append(log_loss(y_train_0,y_pred))

plt.figure()
plt.plot(np.arange(iterations),Loss)
plt.grid(True)
plt.xlabel('Iteration')
plt.ylabel('Class')
plt.show()

print('Training accuracy %.2f'%bin_sgd_clf.score(x_train,y_train_0))
print('Testing accuracy %.2f'%bin_sgd_clf.score(x_test,y_test_0))

y_hat_train_0 = bin_sgd_clf.predict(x_train)

cm_display = ConfusionMatrixDisplay.from_predictions(y_train_0, y_hat_train_0, values_format='.5g')
plt.show()

print(classification_report(y_train_0, y_hat_train_0))

estimator = SGDClassifier(loss='log', penalty='l2', max_iter=100, alpha=0, eta0=0.01, learning_rate='constant', random_state=1729)
cv_bin_clf = cross_validate(estimator, x_train, y_train_0, cv=5, scoring=['precision','recall','f1'], return_train_score=True, return_estimator=True)
pprint(cv_bin_clf)

weight = bin_sgd_clf.coef_
bias = bin_sgd_clf.intercept_
print('dimension of the weights: {0}'.format(weight.shape))
print('bias: {0}'.format(bias))

plt.figure()
plt.plot(np.arange(0,784),weight[0,:])
plt.xlabel('Feature index')
plt.ylabel('Weight value')
plt.ylim(np.min(weight)-5, np.max(weight)+5)
plt.grid()

num_zero = weight.shape[1]-np.count_nonzero(weight)
print('Number of weights with  value zero:%f'%num_zero)

bin_sgd_clf_l2 = SGDClassifier(loss='log', penalty='l2', max_iter=1, warm_start=True, eta0=0.01, alpha=0.01, learning_rate='constant', random_state=1729)
Loss=[]
iterations=100
for i in range(iterations):
  bin_sgd_clf_l2.fit(x_train, y_train_0)
  y_pred = bin_sgd_clf_l2.predict_proba(x_train)
  Loss.append(log_loss(y_train_0,y_pred))

plt.figure()
plt.plot(np.arange(iterations),Loss)
plt.grid(True)
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.show()

weight = bin_sgd_clf_l2.coef_
bias = bin_sgd_clf.intercept_
print(bias)

plt.figure()
plt.plot(np.arange(0,784),weight[0,:])
plt.xlabel('Feature index')
plt.ylabel('Weight value')
plt.ylim(np.min(weight)-5, np.max(weight)+5)
plt.grid()

num_zero = weight.shape[1]-np.count_nonzero(weight)
print('Number of weights with value zero%f'%num_zero)

print('Training accuracy %.2f'%bin_sgd_clf_l2.score(x_train,y_train_0))
print('Testing accuracy %.2f'%bin_sgd_clf_l2.score(x_train, y_train_0))

y_hat_train_0 = bin_sgd_clf_l2.predict(x_train)
cm_display = ConfusionMatrixDisplay.from_predictions(y_train_0, y_hat_train_0, values_format='.5g')
plt.show()

print(classification_report(y_train_0, y_hat_train_0))

index = 10
plt.imshow(x_test[index,:].reshape(28,28), cmap='gray')
pred = bin_sgd_clf.predict(x_test[index].reshape(1,-1))
plt.title(str(pred))
plt.show()

y_hat_test_0 = bin_sgd_clf.predict(x_test)
num_images = 9
factor= np.int(np.sqrt(factor))
fig,ax = plt.subplots(nrows=factor, ncols=factor, figsize=(8,6))
idx_offset=0
for i in range(factor):
  index = idx_offset+i*(factor)
  for j in range(factor):
    ax[i,j].imshow(x_test[index+j].reshape(28,28), cmap='gray')
    ax[i,j].set_title('Prediction: {0}'.format(str(y_hat_test_0[index+j])))
    ax[i,j].set_axis_off()

indx_0 = np.where(y_test_0==1)

zero_imgs = x_test[indx_0[0]]
zero_lbls = y_hat_test_0[indx_0[0]]
num_images = 9
factor= np.int(np.sqrt(factor))
fig,ax = plt.subplots(nrows=factor, ncols=factor, figsize=(8,6))
idx_offset=0
for i in range(factor):
  index = idx_offset+i*(factor)
  for j in range(factor):
    ax[i,j].imshow(zeroimgs[index+j].reshape(28,28), cmap='gray')
    ax[i,j].set_title('Prediction: {0}'.format(str(zerolbls[index+j])))
    ax[i,j].set_axis_off()

"""Hyper Parameter Tuning"""

lr_grid = loguniform(1e-1,1e-3)

print(lr_grid.rvs(3,random_state=42)) #rvs() is used to print the samples of given size

estimator = SGDClassifier(loss='log', penalty='l2', max_iter=1, warm_start=True, eta0=0.01, alpha=0, learning_rate='constant', random_state=1729)

scores = RandomizedSearchCV(estimator, param_distributions={'eta0':lr_grid},cv=5,scoring=['precision','recall','f1'],n_iter=5,refit='f1')

scores.fit(x_train,y_train_0)



"""Logistic Regression

"""

pipe_logit = make_pipeline(MinMaxScaler(), LogisticRegression(C=np.infty, random_state=1729,solver='lbfgs'))
pipe_logit.fit(x_train,y_train_0)

from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
grid_Cs = [0,1e-4,1e-3,1e-2,1e-1,1.0,10.0,100.0]
scaler = MinMaxScaler()
logreg = LogisticRegression(random_state=1729, C=1.0)
pipe = Pipeline([('scaler', scaler),
                ('regressor',logreg)])
pipe_logit_cv = GridSearchCV(pipe, param_grid={'regressor__C': grid_Cs}, scoring='f1')
pipe_logit_cv.fit(x_train,y_train_0)

pipe_logit_cv.best_params_

pipe_logit_cv.best_score_

pipe_logit_cv.best_estimator_

from sklearn.linear_model import LogisticRegressionCV
estimator = LogisticRegressionCV(random_state=1729, scoring='f1',cv=5)
logit_cv = make_pipeline(MinMaxScaler(), estimator)
logit_cv.fit(x_train, y_train_0)

lr_y_hat_0=pipe_logit.predict(x_test)
lr_gs_y_hat_0=pipe_logit_cv.best_estimator_.predict(x_test)
lr_cv_y_hat_0 = logit_cv.predict(x_test)

PR Curve

precision_lr = precision_score(y_test_0, lr_y_hat_0)
recall_lr = recall_score(y_test_0, lr_y_hat_0)
precision_lr_gs = precision_score(y_test_0, lr_gs_y_hat_0)
recall_lr_gs = recall_score(y_test_0, lr_gs_y_hat_0)
precision_lr_cv = precision_score(y_test_0, lr_cv_y_hat_0)
recall_lr_cv = recall_score(y_test_0, lr_cv_y_hat_0)

from sklearn.metrics import precision_recall_curve
y_scores_lr = pipe_logit.decision_function(x_test)
precisions_lr, recalls_lr, thresholds_lr = precision_recall_curve(y_test_0, y_scores_lr)

y_scores_lr_gs = pipe_logit_cv.decision_function(x_test)
precisions_lr_gs, recalls_lr_gs, thresholds_lr_gs = precision_recall_curve(y_test_0, y_scores_lr_gs)

y_scores_lr_cv = logit_cv.decision_function(x_test)
precisions_lr_cv, recalls_lr_cv, thresholds_lr_cv = precision_recall_curve(y_test_0, y_scores_lr_cv)

"""Curve graphs------missing"""

cm_display = ConfusionMatrixDisplay.from_predictions(y_test_0, lr_y_hat_0, values_format='.5g')
plt.show()

cm_display = ConfusionMatrixDisplay.from_predictions(y_test_0, lr_gs_y_hat_0, values_format='.5g')
plt.show()

cm_display = ConfusionMatrixDisplay.from_predictions(y_test_0, lr_cv_y_hat_0, values_format='.5g')
plt.show()



"""Ridge Classifier"""

y_train_0 = -1*np.ones(len(y_train))
y_test_0 = -1*np.ones(len(y_test))

indx_0 = np.where(y_train == '0')

y_train_0[indx_0]=1
indx_0 = np.where(y_test == '0')
y_test_0[indx_0]=1

estimator = RidgeClassifier(alpha=0)
pipe_ridge = make_pipeline(MinMaxScaler(), estimator)
pipe_ridge.fit(x_train,y_train_0)

y_hat_test_0 = pipe_ridge.predict(x_test)
print(classification_report(y_test_0, y_hat_test_0))

cv_bin_ridge_clf = cross_validate(pipe_ridge, x_train, y_train_0, cv=5, scoring=['precision','recall','f1'], return_train_score=True, return_estimator=True)
pprint(cv_bin_ridge_clf)

best_estimator_id = np.argmax(cv_bin_ridge_clf['train_f1']); best_estimator_id

best_estimator = cv_bin_ridge_clf['estimator'][best_estimator_id]

y_hat_test_0 = best_estimator.predict(x_test)
print(classification_report(y_test_0, y_hat_test_0))

models = (pipe_sgd, pipe_sgd_l2, pipe_logit, pipe_ridge)
titles = ('sgd', 'regularized sgd', 'logit', 'ridge')
plt.figure(figsize=(4,4))
plt.subplots(2,2)
for i in range(0,4):
  w = models[i][1].coef_
  w_matrix = w.reshape(28,28)
  w_matrix[w_matrix<0]=0
  plt.subplot(2,2,i+1)
  plt.imshow(w_matrix, cmap='gray')
  plt.title(titles[i])
  plt.axis('off')
  plt.grid(False)
fig.show()



"""Multiclass Classifier(OneVsAll)"""

estimator = SGDClassifier(loss='log',penalty='l2',warm_start=True,max_iter=1,eta0=0.01,alpha=0,learning_rate='constant',random_state=1729)
pipe_sgd_ovr = make_pipeline(MinMaxScaler(), estimator)

Loss=[]
iterations= 100
for i in range(iterations):
  pipe_sgd_ovr.fit(x_train,y_train)
  y_pred = pipe_sgd_ovr.predict_proba(x_train)
  Loss.append(log_loss(y_train,y_pred))

plt.figure()
plt.plot(np.arange(iterations),Loss)
plt.grid(True)
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.show()

pipe_sgd_ovr[1]

pipe_sgd_ovr[1].coef_.shape

y_hat = pipe_sgd_ovr.predict(x_test); y_hat[:5]

cm_display = ConfusionMatrixDisplay.from_predictions(y_test,y_hat,values_format='.5g')
plt.show()

print(classification_report(y_test,y_hat))

"""Multiclass using Solvers"""

pipe_logit_ovr = make_pipeline(MinMaxScaler(),LogisticRegression(random_state=1729,C=np.infty,solver='lbfgs'))
pipe_logit_ovr.fit(x_train,y_train)

y_hat = pipe_logit_ovr.predict(x_test)
cm_display = ConfusionMatrixDisplay.from_predictions(y_test,y_hat, values_format='.5g')
plt.show()

print(classification_report(y_test,y_hat))

W = pipe_logit_ovr[1].coef_
W = MinMaxScaler().fit_transform(W)
fig,ax = plt.subplots(3,3)
index=1
for i in range(3):
  for j in range(3):
    ax[i][j].imshow(W[index,:].reshape(28,28), cmap='gray')
    ax[i][j].set_title('W{0}'.format(index))
    ax[i][j].set_axis_off()
    index+=1



"""SoftMax Regression

"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
from pprint import pprint
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.datasets import fetch_openml
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score
from sklearn.metrics import make_scorer
from sklearn.linear_model import LogisticRegression,LogisticRegressionCV
np.random.seed(42)
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
mpl.rc('axes',labelsize=14)
mpl.rc('xtick',labelsize=12)
mpl.rc('ytick',labelsize=12)
mpl.rc('figure',figsize=(8,6))

X,y = fetch_openml('mnist_784', version=1, return_X_y=True)

X=X.to_numpy()
y=y.to_numpy()
x_train,x_test,y_train,y_test = X[:60000],X[60000:],y[:60000],y[60000:]

pipe = Pipeline([('scaler', StandardScaler()),
                 ('logreg', LogisticRegression(solver='sag', multi_class='multinomial'))])

pipe.fit(x_train,y_train)

pipe[-1].coef_.shape

pipe[-1].intercept_.shape

pipe[-1].classes_

print(classification_report(y_test,pipe.predict(x_test)))

cm_display = ConfusionMatrixDisplay.from_estimator(pipe, x_test, y_test)
plt.show()

scorer = make_scorer(f1_score, average='micro')
pipe = Pipeline([('scaler', StandardScaler()),
                 ('logreg', LogisticRegressionCV(cv=5, multi_class='multinomial', solver='sag', scoring=scorer, max_iter=100, random_state=1729))])
pipe.fit(x_train, y_train)

pipe[-1].C_

pipe[-1].l1_ratio_

print(classification_report(y_test, pipe.predict(x_test)))

ConfusionMatrixDisplay.from_estimator(pipe, x_tyest, y_test)
plt.show()

"""Support Vector Classifier"""

pipe_1 = Pipeline([('scaler', MinMaxScaler()),
                 ('classifier', SVC(kernel='linear', C=1))])
pipe_1.fit(x_train, y_train.ravel())

from sklearn.model_selection import cross_val_score
acc = cross_val_score(pipe_1, x_train, y_train.ravel(),cv=2)
print('Trainig accuracy: {:.2f} %'.format(acc.mean()*100))

y_pred = pipe_1.predict(x_test)
cm_display = ConfusionMatrixDisplay.from_predictions(y_test,y_pred)
plt.show()

pipe_2 = Pipeline([('scaler', MinMaxScaler()),
                 ('classifier', SVC(kernel='rbf', gamma=0.1, C=1))])
pipe_2.fit(x_train, y_train.ravel())

acc = cross_val_score(pipe_2, x_train, y_train.ravel(),cv=2)
print('Trainig accuracy: {:.2f} %'.format(acc.mean()*100))

y_pred = pipe_2.predict(x_test)
cm_display = ConfusionMatrixDisplay.from_predictions(y_test,y_pred)
plt.show()

from sklearn.model_selection import GridSearchCV
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.fit_transform(x_test)

C_range = np.logspace(-2,10,13)
gamma_range = np.logspace(-9,3,13)
param_grid = dict(gamma=gamma_range, C=C_range)
cv = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=42)
grid = GridSearchCV(SVC(kernel='rbf'), param_grid=param_grid, cv=cv)
grid.fit(x_train, y_train.ravel())

y_pred = grid.predict(x_test)
cm_display=ConfusionMatrixDisplay.from_predictions(y_test, y_pred)
plt.show()

"""Bagging and RandomForest Classifier

"""

cv = ShuffleSplit(n_splits=10, random_state=42, test_size=0.2)

def train_classifiers(estimator, X_train, y_train, cv, name):
  estimator.fit(X_train,y_train)
  cv_train_score = cross_val_score(estimator, X_train, y_train, cv=cv, scoring='f1_macro')
  print(f'On an average {name} model has f1 score of'
  f'{cv_train_score.mean():.3f} +/- {cv_train_score.std():.3f} on the training set')

from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay,confusion_matrix
from sklearn.metrics import classification_report
def eval(estimator, X_test, y_test):
  y_pred = estimator.predict(X_test)
  print(classification_report(y_test,y_pred))

  print('Confusin Matrix')
  disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test,y_pred))
  disp.plot()
  plt.title('Confusion matrix')
  plt.show()

dt_pipeline = Pipeline([('classifier', DecisionTreeClassifier())])
train_classifiers(dt_pipeline, X_train, y_train.ravel(), cv, 'decision tree')

eval(dt_pipeline, X_test, y_test)

bagging_pipeline = Pipeline([('classifier', BaggingClassifier())])
train_classifiers(bagging_pipeline, X_train, y_train.ravel(), cv, 'Bagging Classifier')

eval(bagging_pipeline, X_test, y_test)

random_forest_pipeline = Pipeline([('classifier', RandomForestClassifier())])
train_classifiers(random_forest_pipeline, X_train, y_train.ravel(), cv, 'Random Forest Classifier')

eval(random_forest_pipeline, X_test, y_test)